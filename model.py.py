# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lO_HYexaWuH663gLhO97p90BZt8gd0ou
"""

import pandas as pd
df = pd.read_csv("/content/music_album_reviews.csv")

df.head()

import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download necessary resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

nltk.download('punkt_tab')

# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_text(text):
    # Remove punctuation
    text = text.translate(str.maketrans("", "", string.punctuation))

    # Tokenize text
    tokens = word_tokenize(text)

    # Remove stopwords and lemmatize words
    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

    # Rejoin words into a cleaned sentence
    return " ".join(cleaned_tokens)

# Apply cleaning function to the 'Review' column
df["Cleaned_Review"] = df["Review"].astype(str).apply(clean_text)

# Save cleaned dataset
df.to_csv("cleaned_dataset.csv", index=False)

print("Text cleaning complete! Saved as cleaned_dataset.csv")

documents = df["Cleaned_Review"].tolist()

pip install langchain faiss-cpu sentence-transformers openai chromadb

!pip install langchain-community
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# Initialize embedding model (MiniLM is fast & efficient)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Convert text to embeddings
embeddings = [embedding_model.embed_query(text) for text in documents]

# Create FAISS vector store
vector_store = FAISS.from_texts(documents, embedding_model)

# Save FAISS index
vector_store.save_local("faiss_reviews_index")

!pip install google-generativeai

from langchain.chains import RetrievalQA
from langchain.chains.llm import LLMChain  # Import for LLMChain
from langchain.prompts import PromptTemplate

# ... (your previous code) ...

# Define prompt
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

{context}

Question: {question}
Answer:"""
PROMPT = PromptTemplate(
    template=template, input_variables=["context", "question"]
)

# Create LLMChain
llm_chain = LLMChain(llm=llm, prompt=PROMPT)  # Create LLMChain instance

# Create RAG Chain using from_chain_type
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT} # Pass the prompt to chain_type_kwargs
)

# Example Query
query = "What do people think about OK Computer?"
response = rag_chain.run(query)

print("RAG Response:", response)

